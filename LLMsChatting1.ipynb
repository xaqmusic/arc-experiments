{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b11fa6-0007-47b2-bde1-641cf98fa749",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Let's Get 3 LLMs Chatting with each other..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d85ebf-1f4a-4ca8-9f0f-ca7df56ab4a0",
   "metadata": {},
   "source": [
    "This first version gets a response from each LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235a3d6-b47e-4540-903f-a394372f9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names and colors\n",
    "models = {\n",
    "    \"smollm-360m-instruct-v0.2\": {\"name\": \"Onink\", \"color\": \"\\033[94m\"},\n",
    "    \"smollm-360m-instruct-v0.2:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\"},\n",
    "    \"smollm-360m-instruct-v0.2:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\"}\n",
    "}\n",
    "\n",
    "def chat_with_llms(system_prompt, user_prompt, temperature=0.7):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    for model_id, model_info in models.items():\n",
    "        try:\n",
    "            # Get the response from the current model\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=messages,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Print the response with the model's name and colored text\n",
    "            print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "chat_with_llms(\"You are a little pig talking to your siblings.\", \"What do we do about the big bad wolf?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ffd60-304c-4a41-b837-2dbcdefe3a3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM's talking over each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1c797-954b-4c73-b003-881d9d0acdfc",
   "metadata": {},
   "source": [
    "All LLm's responsd at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eafc44-4f89-44de-bac5-7d3c682af95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names and colors\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompt, user_prompt, temperature=0.95):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    message_queue.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    while True:\n",
    "        new_messages = []\n",
    "        \n",
    "        for model_id, model_info in models.items():\n",
    "            try:\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue + messages,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "                \n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": completion.choices[0].message.content.strip(),\n",
    "                    \"model_name\": model_info['name']\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "        \n",
    "        # Add new messages to the shared message queue\n",
    "        message_queue.extend(new_messages)\n",
    "        \n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "chat_with_llms(\"You are a little pig\", \"We are walking down a path in the forest looking for something fun to do. Be sure to mention your brother's name when you hear his reply. Only respond with a single sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1cb04-3d56-4977-8478-2ee7cb02b666",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sequential Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c97a6-bb2f-44f4-bc0b-93c13fe5f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a helpful assistant.\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are an expert in programming.\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a creative artist.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_id = model_order[current_model_index]\n",
    "            model_info = models[model_id]\n",
    "\n",
    "            # Get the response from the current model\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=message_queue,\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "            # Print the response with the model's name and colored text\n",
    "            print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "            # Add the model's response to the shared message queue for other models\n",
    "            new_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": completion.choices[0].message.content.strip(),\n",
    "                \"model_name\": model_info['name']\n",
    "            }\n",
    "            message_queue.append(new_message)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "\n",
    "        # Move to the next model\n",
    "        current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "system_prompts = {\n",
    "    \"llama-3.2-1b-instruct\": \"You are the little piggy named Oink.  You are the leader of the group of three little pigs but get distracted easily by noises.  Your younger brothers are Boink and Doink.\",\n",
    "    \"llama-3.2-1b-instruct:2\": \"You are the little piggy named Boink. Your brothers are Oink and Doink.  You love mud and things on under ground.\",\n",
    "    \"llama-3.2-1b-instruct:3\": \"You are the little piggy named Doink.  You are the youngest of the three little pigs.  Oink and Boink are your older brothers.  You are shy but observant and like things in the sky.\"\n",
    "}\n",
    "\n",
    "chat_with_llms(system_prompts, \"You and your other piggy brothers are walking down a path in the forest looking for something fun to do.  You are talking to your brothers so form your reply as dialog.  Reply directly to your brothers' ideas. Only respond with a single sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde61c55-d3d6-4a33-a1bf-32a180e5f61e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# You are part of the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329f554-85df-4612-b630-87c60cc86fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"Please enter your name: \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a helpful assistant.\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are an expert in programming.\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a creative artist.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "        \n",
    "    message_queue = []\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": completion.choices[0].message.content.strip(),\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": user_reply})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        # time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    system_prompts = {\n",
    "        \"llama-3.2-1b-instruct\": f\"You are the little piggy named Oink.  You are the leader of the group of three little pigs and like to keep moving.  Your younger brothers are Boink and Doink. You are travelling with {user_name} and you always pay close attention when they say something\",\n",
    "        \"llama-3.2-1b-instruct:2\": f\"You are the little piggy named Boink. You are Oink's younger brother and Doink's older brother.  You love mud and things on under ground. You are travelling with {user_name} and you always pay close attention when they say something.\",\n",
    "        \"llama-3.2-1b-instruct:3\": f\"You are the little piggy named Doink.  You are the youngest of the three little pigs.  Oink and Boink are your older brothers.  You are shy but observant. You are travelling with {user_name} and you always pay close attention when they say something.\"\n",
    "    }\n",
    "\n",
    "    chat_with_llms(system_prompts, f\"We are creating a story by responding to each other.  In this story, you, your two other brothers, and a friend named {user_name} are on an adventure in the woods.  Respond with your action and your dialog.  Respond only as yourself. Try to include {user_name} as much as possible in the adventure.  Keep your responses very short with only action and dialog.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1bdfc-da6c-4939-a95d-312d329a36ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Better Clarity Between Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb4f5e-e050-4871-b98e-2703505f549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"Please enter your name: \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts*** ALOS PUT THESE IN THE SYSTEM PROMPTS AREA\n",
    "models = {\n",
    "    \"gemma-2-2b-it\": {\"name\": \"Oink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"},\n",
    "    \"gemma-2-2b-it:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"},\n",
    "    \"gemma-2-2b-it:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "        \n",
    "    message_queue = []\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[{model_info['name']}]: {completion.choices[0].message.content.strip()}\",\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": f\"[{user_name}]: {user_reply}\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        # time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    global user_name\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    system_prompts = {\n",
    "        \"gemma-2-2b-it\": f\"You are a little piggy named Oink.  You are the leader of a group of three little pigs and like puzzles and adventures.  Your younger brothers are Boink and Doink. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\",\n",
    "        \"gemma-2-2b-it:2\": f\"You are a little piggy named Boink and one of the three little pigs. You are Oink's younger brother and Doink's older brother.  You love mud and things on under ground. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\",\n",
    "        \"gemma-2-2b-it:3\": f\"You are a little piggy named Doink and one of the three little pigs.  You are the youngest brother. Oink and Boink are your older brothers.  You are shy but observant. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\"\n",
    "    }\n",
    "\n",
    "    chat_with_llms(system_prompts, \"You and your other piggy brothers are walking down a path in the forest looking for something fun to do.  Reply directly their ideas and try to add something interesting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681922-e2e9-41d6-9674-44e812e8a1a0",
   "metadata": {},
   "source": [
    "# Let's add a narrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ed74e-f7e0-41bb-8926-d4e51b68e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"What is your name, brave adventurer? \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    # \\/ enter the name of the models you have loaded here.  gemma-2-2b-it is recommended because it is small, fun, and safe for kids.\n",
    "    \"gemma-2-2b-it\": {\"name\": \"Oink\", \"color\": \"\\033[94m\", \"system_prompt\": f\"You are a little piggy named Oink.\"},\n",
    "    \"gemma-2-2b-it:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": f\"You are a little piggy named Boink.\"},\n",
    "    \"gemma-2-2b-it:3\": {\"name\": \"NARRATOR\", \"color\": \"\\033[95m\", \"system_prompt\": f\"You are the narrator of this story and respond with what is happening in the scene based on the inputs you get. You are responsible for building an interesting environment for the user to interact with. Respond in the 2nd person style of Zork only. Describe the scene for the user and provide any NPC dialog in quotes. Be descriptive about the scene. End every phrase with 'What do you want to do now?'.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(user_prompt, temperature=0.95):\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Create messages list with individual system prompt and previous conversation history\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": model_info[\"system_prompt\"]}\n",
    "                ]\n",
    "                messages.extend(message_queue)\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[{model_info['name']}]: {completion.choices[0].message.content.strip()}\",\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": f\"[{user_name}]: {user_reply}\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    global user_name\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    # Update system prompts with user name\n",
    "    models[\"gemma-2-2b-it\"][\"system_prompt\"] = f\"You are a little piggy named Oink. You are the leader of a group of three little pigs and like puzzles and adventures. You love to describe what you see in great detail. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\"\n",
    "    models[\"gemma-2-2b-it:2\"][\"system_prompt\"] = f\"You are a little piggy named Boink and one of the three little pigs. You love to suggest new ideas and talk in short rhymes. You always respond after your brother pig, Oink, and add fun details that you think he missed.  You love to use emojis! The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\"\n",
    "    \n",
    "    chat_with_llms(\"We are on an adventure in a wonderous cave!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a355f-aebb-4e65-ab0b-cf44c1e3eede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
