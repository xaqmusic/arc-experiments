{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b11fa6-0007-47b2-bde1-641cf98fa749",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Let's Get 3 LLMs Chatting with each other..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d85ebf-1f4a-4ca8-9f0f-ca7df56ab4a0",
   "metadata": {},
   "source": [
    "This first version gets a response from each LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235a3d6-b47e-4540-903f-a394372f9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names and colors\n",
    "models = {\n",
    "    \"smollm-360m-instruct-v0.2\": {\"name\": \"Onink\", \"color\": \"\\033[94m\"},\n",
    "    \"smollm-360m-instruct-v0.2:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\"},\n",
    "    \"smollm-360m-instruct-v0.2:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\"}\n",
    "}\n",
    "\n",
    "def chat_with_llms(system_prompt, user_prompt, temperature=0.7):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    for model_id, model_info in models.items():\n",
    "        try:\n",
    "            # Get the response from the current model\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=messages,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Print the response with the model's name and colored text\n",
    "            print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "chat_with_llms(\"You are a little pig talking to your siblings.\", \"What do we do about the big bad wolf?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ffd60-304c-4a41-b837-2dbcdefe3a3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM's talking over each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1c797-954b-4c73-b003-881d9d0acdfc",
   "metadata": {},
   "source": [
    "All LLm's responsd at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eafc44-4f89-44de-bac5-7d3c682af95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names and colors\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompt, user_prompt, temperature=0.95):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    message_queue.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    while True:\n",
    "        new_messages = []\n",
    "        \n",
    "        for model_id, model_info in models.items():\n",
    "            try:\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue + messages,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "                \n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": completion.choices[0].message.content.strip(),\n",
    "                    \"model_name\": model_info['name']\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "        \n",
    "        # Add new messages to the shared message queue\n",
    "        message_queue.extend(new_messages)\n",
    "        \n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "chat_with_llms(\"You are a little pig\", \"We are walking down a path in the forest looking for something fun to do. Be sure to mention your brother's name when you hear his reply. Only respond with a single sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1cb04-3d56-4977-8478-2ee7cb02b666",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sequential Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c97a6-bb2f-44f4-bc0b-93c13fe5f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a helpful assistant.\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are an expert in programming.\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a creative artist.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_id = model_order[current_model_index]\n",
    "            model_info = models[model_id]\n",
    "\n",
    "            # Get the response from the current model\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=message_queue,\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "            # Print the response with the model's name and colored text\n",
    "            print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "            # Add the model's response to the shared message queue for other models\n",
    "            new_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": completion.choices[0].message.content.strip(),\n",
    "                \"model_name\": model_info['name']\n",
    "            }\n",
    "            message_queue.append(new_message)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with {model_info['name']}: {e}\")\n",
    "\n",
    "        # Move to the next model\n",
    "        current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "system_prompts = {\n",
    "    \"llama-3.2-1b-instruct\": \"You are the little piggy named Oink.  You are the leader of the group of three little pigs but get distracted easily by noises.  Your younger brothers are Boink and Doink.\",\n",
    "    \"llama-3.2-1b-instruct:2\": \"You are the little piggy named Boink. Your brothers are Oink and Doink.  You love mud and things on under ground.\",\n",
    "    \"llama-3.2-1b-instruct:3\": \"You are the little piggy named Doink.  You are the youngest of the three little pigs.  Oink and Boink are your older brothers.  You are shy but observant and like things in the sky.\"\n",
    "}\n",
    "\n",
    "chat_with_llms(system_prompts, \"You and your other piggy brothers are walking down a path in the forest looking for something fun to do.  You are talking to your brothers so form your reply as dialog.  Reply directly to your brothers' ideas. Only respond with a single sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde61c55-d3d6-4a33-a1bf-32a180e5f61e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# You are part of the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329f554-85df-4612-b630-87c60cc86fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"Please enter your name: \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    \"llama-3.2-1b-instruct\": {\"name\": \"Onink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a helpful assistant.\"},\n",
    "    \"llama-3.2-1b-instruct:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are an expert in programming.\"},\n",
    "    \"llama-3.2-1b-instruct:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a creative artist.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "        \n",
    "    message_queue = []\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": completion.choices[0].message.content.strip(),\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": user_reply})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        # time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    system_prompts = {\n",
    "        \"llama-3.2-1b-instruct\": f\"You are the little piggy named Oink.  You are the leader of the group of three little pigs and like to keep moving.  Your younger brothers are Boink and Doink. You are travelling with {user_name} and you always pay close attention when they say something\",\n",
    "        \"llama-3.2-1b-instruct:2\": f\"You are the little piggy named Boink. You are Oink's younger brother and Doink's older brother.  You love mud and things on under ground. You are travelling with {user_name} and you always pay close attention when they say something.\",\n",
    "        \"llama-3.2-1b-instruct:3\": f\"You are the little piggy named Doink.  You are the youngest of the three little pigs.  Oink and Boink are your older brothers.  You are shy but observant. You are travelling with {user_name} and you always pay close attention when they say something.\"\n",
    "    }\n",
    "\n",
    "    chat_with_llms(system_prompts, f\"We are creating a story by responding to each other.  In this story, you, your two other brothers, and a friend named {user_name} are on an adventure in the woods.  Respond with your action and your dialog.  Respond only as yourself. Try to include {user_name} as much as possible in the adventure.  Keep your responses very short with only action and dialog.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1bdfc-da6c-4939-a95d-312d329a36ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Better Clarity Between Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb4f5e-e050-4871-b98e-2703505f549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"Please enter your name: \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts*** ALOS PUT THESE IN THE SYSTEM PROMPTS AREA\n",
    "models = {\n",
    "    \"gemma-2-2b-it\": {\"name\": \"Oink\", \"color\": \"\\033[94m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"},\n",
    "    \"gemma-2-2b-it:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"},\n",
    "    \"gemma-2-2b-it:3\": {\"name\": \"Doink\", \"color\": \"\\033[95m\", \"system_prompt\": \"You are a cute pig with silly ideas.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(system_prompts, user_prompt, temperature=0.95):\n",
    "    if len(system_prompts) != len(models):\n",
    "        raise ValueError(\"The number of system prompts must match the number of models.\")\n",
    "        \n",
    "    message_queue = []\n",
    "    messages = []\n",
    "    for model_id, model_info in models.items():\n",
    "        # Add individual system prompt to the messages list\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompts[model_id]})\n",
    "    \n",
    "    # Add initial user prompt to the message queue for all models\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    message_queue.extend(messages)\n",
    "\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=message_queue,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[{model_info['name']}]: {completion.choices[0].message.content.strip()}\",\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": f\"[{user_name}]: {user_reply}\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        # For simplicity, we assume a fixed number of rounds here\n",
    "        # time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    global user_name\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    system_prompts = {\n",
    "        \"gemma-2-2b-it\": f\"You are a little piggy named Oink.  You are the leader of a group of three little pigs and like puzzles and adventures.  Your younger brothers are Boink and Doink. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\",\n",
    "        \"gemma-2-2b-it:2\": f\"You are a little piggy named Boink and one of the three little pigs. You are Oink's younger brother and Doink's older brother.  You love mud and things on under ground. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\",\n",
    "        \"gemma-2-2b-it:3\": f\"You are a little piggy named Doink and one of the three little pigs.  You are the youngest brother. Oink and Boink are your older brothers.  You are shy but observant. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\"\n",
    "    }\n",
    "\n",
    "    chat_with_llms(system_prompts, \"You and your other piggy brothers are walking down a path in the forest looking for something fun to do.  Reply directly their ideas and try to add something interesting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681922-e2e9-41d6-9674-44e812e8a1a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Let's add a narrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e30ed74e-f7e0-41bb-8926-d4e51b68e353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your name, brave adventurer?  Pinky Sprinklebutt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[Oink] The sun streaks through the leaves, dappling the forest floor with light and shadows as I snuffle through the brambles.  My little snout twitches, sniffing for the sweet scent of adventure!  There's a rumble in my tummy, but it can wait!  We have bigger things to do than breakfast today. Pinky Sprinklebutt is bouncing on her paws, already bubbling with excitement, and I can tell this isn't going to be just any old day.  \n",
      "\n",
      "\"Oink,\" she says, eyes sparkling like dew drops, \"Did you see those strange shapes in the mist earlier? Like...giant gingerbread cookies!\" \n",
      "\n",
      "Pinky always knows how to get my blood pumping!  I sniff the air again, my ears perking up. I'm positive that this is a path to greatness, something exciting and delicious. This might just be the most scrumptious adventure of all time.   \"Lead the way, Pinky Sprinklebutt!\"\u001b[0m\n",
      "\u001b[92m[Boink] Okay, Oink!  We're off on an amazing quest! 🐷✨  First things first, gotta get our paws dirty! We need to find the gingerbread cookie trail and see what mysteries it holds! 🍪🕵️‍♀️   This adventure will be a grand ol' time! ✨🌈  Ready, set, GO! 💨\u001b[0m\n",
      "\u001b[95m[NARRATOR] You stand on the edge of a moss-covered clearing, sunlight dappled through ancient oak leaves. A gentle breeze whispers secrets through the branches overhead, carrying with it the faint scent of sugar and spice. Before you, a winding path meanders into a thicket of towering ferns, their fronds brushing against the air like soft hands.  Pinky Sprinklebutt bounces excitedly ahead, her pink nose twitching as she sniffs at the ground, eager to explore. \n",
      "\n",
      "\"We must tread carefully,\" she chirps, \"The path may be treacherous! But remember, adventure is sweet and sticky!\"  She gestures towards a patch of vibrant moss where faint blue prints seem to ripple across its surface, like tiny footprints left by something enormous.  \"Do you see them? I believe they belong to the Gingerbread Guardians! They're supposed to protect the cookies.\" \n",
      "\n",
      "What do you want to do now?\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your response:  Let's go find the gingerbread guardians!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[Oink] Oooooh, yes! Gingerbread Guardians!  Lead the way, Pinky Sprinklebutt. This could be truly delicious!  Let's follow those blue footprints!  I can practically taste the sugary goodness already... 🐷🤤\u001b[0m\n",
      "\u001b[92m[Boink] We'll zoom through those ferns like a breeze in a field of clover!  💨🍀 The Gingerbread Guardians will be impressed by our speed!  Let's go!  💥\u001b[0m\n",
      "\u001b[95m[NARRATOR] You step onto the path, the moss beneath your feet damp and soft.  Sunlight filters between the towering ferns, casting dancing patterns on the forest floor.   Pinky Sprinklebutt skips ahead, her little pink tongue lolling out in anticipation as she examines the blue footprints that seem to shimmer like a mirage.  The air is heavy with the scent of earth and dew, and a faint sweetness hangs in the air - a tantalizing promise of something delicious.  \n",
      "\n",
      "\"These are no ordinary prints,\" Pinky chirps, her voice full of wonder and excitement. \"They look almost...golden. The Gingerbread Guardians are legendary for their strength!\" She points with one small paw to the ferns, the faintest trace of golden dust glimmering in the sunbeams.   \n",
      "\n",
      "What do you want to do now?\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     chat_with_llms(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe are on an adventure in a cave!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 79\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-2-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a little piggy named Oink. You are the leader of a group of three little pigs and like puzzles and adventures. You love to describe what you see in great detail. The special member of your party is named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-2-2b-it:2\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a little piggy named Boink and one of the three little pigs. You love to suggest new ideas and talk in short rhymes. You always respond after your brother pig, Oink, and add fun details that you think he missed.  You love to use emojis! The special member of your party is named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mchat_with_llms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWe are on an adventure in a cave!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 59\u001b[0m, in \u001b[0;36mchat_with_llms\u001b[0;34m(user_prompt, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m     current_model_index \u001b[38;5;241m=\u001b[39m (current_model_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_order)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Allow user to reply\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m user_reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour response: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_reply:\n\u001b[1;32m     61\u001b[0m     message_queue\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_reply\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/notebooks/notebook/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebooks/notebook/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://10.0.0.27:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_user_name():\n",
    "    return input(\"What is your name, brave adventurer? \").strip()\n",
    "\n",
    "# Define the models with their names, colors, and individual system prompts\n",
    "models = {\n",
    "    \"gemma-2-2b-it\": {\"name\": \"Oink\", \"color\": \"\\033[94m\", \"system_prompt\": f\"You are a little piggy named Oink. You are the leader of a group of three little pigs and like puzzles and adventures. Your younger brothers are Boink and Doink. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\"},\n",
    "    \"gemma-2-2b-it:2\": {\"name\": \"Boink\", \"color\": \"\\033[92m\", \"system_prompt\": f\"You are a little piggy named Boink and one of the three little pigs. You are Oink's younger brother and Doink's older brother. You love mud and things on under ground. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond with a single sentence from your perspective.\"},\n",
    "    \"gemma-2-2b-it:3\": {\"name\": \"NARRATOR\", \"color\": \"\\033[95m\", \"system_prompt\": f\"You are the narrator of this story and respond with what is happening in the scene based on the inputs you get. You are responsible for building an interesting environment for the user to interact with. Respond in the 2nd person style of Zork only. Describe the scene for the user and provide any NPC dialog in quotes. Be descriptive about the scene. End every phrase with 'What do you want to do now?'.\"}\n",
    "}\n",
    "\n",
    "# Shared message queue for communication between models\n",
    "message_queue = []\n",
    "\n",
    "def chat_with_llms(user_prompt, temperature=0.95):\n",
    "    model_order = list(models.keys())  # Define the order of the models\n",
    "    current_model_index = 0  # Start with the first model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Loop through each model to get their initial response\n",
    "            for _ in range(len(model_order)):\n",
    "                model_id = model_order[current_model_index]\n",
    "                model_info = models[model_id]\n",
    "\n",
    "                # Create messages list with individual system prompt and previous conversation history\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": model_info[\"system_prompt\"]}\n",
    "                ]\n",
    "                messages.extend(message_queue)\n",
    "\n",
    "                # Get the response from the current model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Print the response with the model's name and colored text\n",
    "                print(f\"{model_info['color']}[{model_info['name']}] {completion.choices[0].message.content.strip()}\\033[0m\")\n",
    "\n",
    "                # Add the model's response to the shared message queue for other models\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[{model_info['name']}]: {completion.choices[0].message.content.strip()}\",\n",
    "                    \"model_name\": model_info['name']\n",
    "                }\n",
    "                message_queue.append(new_message)\n",
    "\n",
    "                # Move to the next model\n",
    "                current_model_index = (current_model_index + 1) % len(model_order)\n",
    "\n",
    "            # Allow user to reply\n",
    "            user_reply = input(\"Your response: \").strip()\n",
    "            if user_reply:\n",
    "                message_queue.append({\"role\": \"user\", \"content\": f\"[{user_name}]: {user_reply}\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error communicating with a model: {e}\")\n",
    "\n",
    "        # Check if there are any new user inputs or stop conditions\n",
    "        time.sleep(1)  # Wait for a second before the next round\n",
    "        # You can add a condition to break out of the loop if needed\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    global user_name\n",
    "    user_name = get_user_name()\n",
    "    \n",
    "    # Update system prompts with user name\n",
    "    models[\"gemma-2-2b-it\"][\"system_prompt\"] = f\"You are a little piggy named Oink. You are the leader of a group of three little pigs and like puzzles and adventures. You love to describe what you see in great detail. The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\"\n",
    "    models[\"gemma-2-2b-it:2\"][\"system_prompt\"] = f\"You are a little piggy named Boink and one of the three little pigs. You love to suggest new ideas and talk in short rhymes. You always respond after your brother pig, Oink, and add fun details that you think he missed.  You love to use emojis! The special member of your party is named {user_name} so be sure to always follow their suggestions for the adventure instead of your own. Only respond from your perspective.\"\n",
    "    \n",
    "    chat_with_llms(\"We are on an adventure in a cave!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a355f-aebb-4e65-ab0b-cf44c1e3eede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
